# -*- coding: utf-8 -*-
"""Vector_database.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K_XpZ6JRdc2811aVpU0XhW9WMN8qUAn9
"""

# import Libraries

import openai
import langchain
import pinecone
from langchain.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Pinecone
from langchain.llms import OpenAI

# !pip install langchain
# !pip install langchain-community
# !pip install pinecone
# !pip install openai
# !pip install pdfplumber
# !pip install tiktoken

import os
os.environ["OPENAI_API_KEY"] = grisha_key

## Lets Read the document
def read_doc(directory):
    file_loader=PyPDFDirectoryLoader(directory)
    documents=file_loader.load()
    return documents

import os
import pdfplumber


def read_doc(file_path):
    all_text = ""
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if text:
                all_text += text
    return all_text

doc=read_doc('budget_speech.pdf')
len(doc)


from langchain.schema import Document

# def chunk_data(docs,chunk_size=800,chunk_overlap=50):
#        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
#        chunks = text_splitter.split_documents([Document(page_content=docs)])
#        return chunks



from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
def chunk_data(docs):
       text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
       chunks = text_splitter.split_documents([Document(page_content=doc)])
       return chunks

documents=chunk_data(docs=doc)
len(documents)

## Embedding Technique Of OPENAI
embeddings=OpenAIEmbeddings()
embeddings

vectors=embeddings.embed_query("How are you?")
len(vectors)

## Vector Search DB In Pinecone
pinecone.init(
    api_key="923d5299-ab4c-4407-bfe6-7f439d9a9cb9",
    environment="gcp-starter"
)
index_name="langchainvector"

index=Pinecone.from_documents(doc,embeddings,index_name=index_name)



## Cosine Similarity Retreive Results from VectorDB
def retrieve_query(query,k=2):
    matching_results=index.similarity_search(query,k=k)
    return matching_results

from langchain.chains.question_answering import load_qa_chain
from langchain import OpenAI

llm=OpenAI(model_name="text-davinci-003",temperature=0.5)
chain=load_qa_chain(llm,chain_type="stuff")

## Search answers from VectorDB
def retrieve_answers(query):
    doc_search=retrieve_query(query)
    print(doc_search)
    response=chain.run(input_documents=doc_search,question=query)
    return response

our_query = "How much the agriculture target will be increased by how many crore?"
answer = retrieve_answers(our_query)
print(answer)



# !pip install faiss-cpu

